{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffn2KeXHrk1s",
        "outputId": "03c4beeb-1e4e-4bf1-931b-6a54a047f49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural\n",
            "language\n",
            "processing\n",
            ",\n",
            "including\n",
            "sentiment\n",
            "analysis\n",
            "and\n",
            "topic\n",
            "modelling\n",
            ".\n",
            "SpaCy\n",
            "is\n",
            "fast\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Loading the language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"Natural language processing, including sentiment analysis and topic modelling. SpaCy is fast.\"\n",
        "\n",
        "# Applying tokenisation\n",
        "doc = nlp(text)\n",
        "\n",
        "# Taking tokens (words and punctuation) out of the text\n",
        "for token in doc:\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying sentence splitting\n",
        "doc = nlp(text)\n",
        "\n",
        "# Taking sentences out of the text\n",
        "for sentence in doc.sents:\n",
        "    print(sentence.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GxWni90r9KS",
        "outputId": "2a80b4d6-bd47-4d18-b8a7-e97420fceade"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing, including sentiment analysis and topic modelling.\n",
            "SpaCy is fast.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the analysis\n",
        "doc = nlp(text)\n",
        "\n",
        "# Bringing out words and their parts of speech\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1weCuE6sL6B",
        "outputId": "f8bab939-c150-4328-99dd-f47dab2675f1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural ADJ\n",
            "language NOUN\n",
            "processing NOUN\n",
            ", PUNCT\n",
            "including VERB\n",
            "sentiment NOUN\n",
            "analysis NOUN\n",
            "and CCONJ\n",
            "topic NOUN\n",
            "modelling NOUN\n",
            ". PUNCT\n",
            "SpaCy PROPN\n",
            "is AUX\n",
            "fast ADJ\n",
            ". PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bringing words back to their base form Lemmatisation is the process of bringing a word back to its base form (lemma) by removing endings and suffixes. This helps to harmonise the different forms of a word and improve the accuracy of the analysis\n"
      ],
      "metadata": {
        "id": "YR2tZAtzsO5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFE_qMEasQ_c",
        "outputId": "7931b81c-fbfb-4cde-f4c3-01d0bfaae3b7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural natural\n",
            "language language\n",
            "processing processing\n",
            ", ,\n",
            "including include\n",
            "sentiment sentiment\n",
            "analysis analysis\n",
            "and and\n",
            "topic topic\n",
            "modelling modelling\n",
            ". .\n",
            "SpaCy SpaCy\n",
            "is be\n",
            "fast fast\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detecting and classifying Named Entities Named Entities are real world objects that can be identified by name, such as names of people, places, dates, organisations, etc. Extracting and classifying named entities is an important task in NLP. SpaCy provides convenient tools for this purpose."
      ],
      "metadata": {
        "id": "JUCCgs8wsnkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Apple is going to build a new office in London in 2023.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLK55zh5sqZ7",
        "outputId": "cb47467e-62db-4501-c9d6-dd51548df4b0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "London GPE\n",
            "2023 DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Loading the language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Preparing the text corpus\n",
        "corpus = [\"I like cats.\", \"Dogs are friendly.\", \"Cats and dogs are pets.\"]\n",
        "\n",
        "# # Tokenise and lemmatise the text\n",
        "processed_corpus = []\n",
        "for doc in nlp.pipe(corpus):\n",
        "    processed_corpus.append([token.lemma_ for token in doc])\n",
        "\n",
        "# Teaching modelling—å Word2Vec\n",
        "model = Word2Vec(processed_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"custom_word_vectors.model\")"
      ],
      "metadata": {
        "id": "wRs_eoyxMv7n"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Loading the model Word2Vec\n",
        "custom_model = Word2Vec.load(\"custom_word_vectors.model\")\n",
        "\n",
        "# We get the vector representation of the word \"cat\"\n",
        "vector_cat = custom_model.wv[\"cat\"]\n",
        "\n",
        "# We get the vector representation of the word \"dog\"\n",
        "vector_dog = custom_model.wv[\"dog\"]\n",
        "\n",
        "# Preparing data for classification\n",
        "X = [vector_cat, vector_dog]\n",
        "y = [\"animal\", \"animal1\"]\n",
        "\n",
        "# Training the classification model\n",
        "classifier = SVC()\n",
        "classifier.fit(X, y)\n",
        "\n",
        "# Testing the model\n",
        "test_vector = custom_model.wv[\"dog\"]\n",
        "predicted_label = classifier.predict([test_vector])[0]\n",
        "\n",
        "print(\"Predicted label for 'dog':\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsHm6IE0uM9b",
        "outputId": "213dac39-22bd-47c1-aac8-9fbd948e6a70"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label for 'dog': animal1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntactic Analysis Dependency Tree Dependency Tree Structure Syntactic analysis, also known as dependency analysis, is an important part of natural language processing. It focuses on identifying syntactic relationships between words in a sentence. To visualise these relationships, a dependency tree, which is a graphical representation of sentence structure, is used.\n",
        "\n",
        "Using a dependency tree to analyse the relationships between words A dependency tree allows us to easily see which words are main words and which are dependent words, as well as which syntactic relations connect them. In spaCy, you can obtain a dependency tree for a sentence using the .print_tree()."
      ],
      "metadata": {
        "id": "mwz0qznU5Hp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural language processing, including sentiment analysis and topic modelling. SpaCy is fast.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Display the dependency tree\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V60NyxA5JvR",
        "outputId": "d9a20044-3e10-4ed2-d3c5-b45bfd89b5c9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural amod language\n",
            "language compound processing\n",
            "processing ROOT processing\n",
            ", punct processing\n",
            "including prep processing\n",
            "sentiment compound analysis\n",
            "analysis pobj including\n",
            "and cc analysis\n",
            "topic compound modelling\n",
            "modelling conj analysis\n",
            ". punct processing\n",
            "SpaCy nsubj is\n",
            "is ROOT is\n",
            "fast acomp is\n",
            ". punct is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grammatical relations The concept of syntactic relations (subject, object, etc.) Syntactic relations define how words are related to each other in a sentence. Some of the key syntactic relations include subject, object, direct complement, indirect complement, etc. These relations help us to understand the semantic structure of a sentence."
      ],
      "metadata": {
        "id": "kRFbz2Om5e7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural language processing, including sentiment analysis and topic modelling. SpaCy is fast.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extracting grammatical relations and semantic roles\n",
        "for token in doc:\n",
        "    if token.dep_ == \"nsubj\":\n",
        "        print(f\"Subject: {token.text}\")\n",
        "    elif token.dep_ == \"dobj\":\n",
        "        print(f\"Direct Object: {token.text}\")\n",
        "    elif token.dep_ == \"prep\":\n",
        "        print(f\"Preposition: {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEH1UZT45gMj",
        "outputId": "b5ebeccf-fd82-4127-e34d-d3501d81ca10"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preposition: including\n",
            "Subject: SpaCy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text tone analysis Text tone analysis is an important component of analysing sentiment and opinion in text data. SpaCy, although not a dedicated library for tone analysis, can be a useful tool for pre-processing and analysing texts before applying specialised techniques.\n"
      ],
      "metadata": {
        "id": "LsmT6Pnt55Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Natural language processing, including sentiment analysis and topic modelling. SpaCy is fast.I love this product. It's amazing!\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Let's use TextBlob to analyse tonality\n",
        "analysis = TextBlob(text)\n",
        "\n",
        "# Evaluate the mood of the text\n",
        "sentiment = analysis.sentiment.polarity\n",
        "\n",
        "if sentiment > 0:\n",
        "    sentiment_label = \"positive\"\n",
        "elif sentiment < 0:\n",
        "    sentiment_label = \"negative\"\n",
        "else:\n",
        "    sentiment_label = \"neutral\"\n",
        "\n",
        "print(f\"Sentiment: {sentiment_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtqPGpyq57Y7",
        "outputId": "905d3af5-5ab5-4bf9-8e30-65cc5bd80a64"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting keywords from text helps to condense information and highlight the most important aspects of the content. SpaCy provides the ability to extract keywords using word frequency or semantic meaning.\n"
      ],
      "metadata": {
        "id": "U9Gcj4h66LzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Natural language processing is a field of study focused on making sense of text data.Natural language processing, including sentiment analysis and topic modelling. SpaCy is fast.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract keywords based on frequency\n",
        "keywords_freq = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
        "# Extract keywords based on the weight of embedded vectors\n",
        "keywords_semantic = [token.text for token in doc if not token.is_stop and token.vector_norm > 0]\n",
        "\n",
        "print(\"Keywords based on frequency:\", keywords_freq)\n",
        "print(\"Keywords based on semantics:\", keywords_semantic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIgteZyB6RuL",
        "outputId": "4beef001-89e0-4f96-beef-509262f3eb89"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keywords based on frequency: ['Natural', 'language', 'processing', 'field', 'study', 'focused', 'making', 'sense', 'text', 'data', 'Natural', 'language', 'processing', 'including', 'sentiment', 'analysis', 'topic', 'modelling', 'SpaCy', 'fast']\n",
            "Keywords based on semantics: ['Natural', 'language', 'processing', 'field', 'study', 'focused', 'making', 'sense', 'text', 'data', '.', 'Natural', 'language', 'processing', ',', 'including', 'sentiment', 'analysis', 'topic', 'modelling', '.', 'SpaCy', 'fast', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ù–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª"
      ],
      "metadata": {
        "id": "YCGpwz7rQi69"
      }
    }
  ]
}